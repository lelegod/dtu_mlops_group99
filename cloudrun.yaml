steps:
  # STEP 0: Optimization - Pull existing images for layer caching
  - name: 'gcr.io/cloud-builders/docker'
    id: 'Pull Cache'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        docker pull $_REGION-docker.pkg.dev/$PROJECT_ID/dtu-mlops-images/train-image:latest || exit 0
        docker pull $_REGION-docker.pkg.dev/$PROJECT_ID/dtu-mlops-images/api-image:latest || exit 0
        docker pull $_REGION-docker.pkg.dev/$PROJECT_ID/dtu-mlops-images/frontend-image:latest || exit 0

  # STEP 1-3: Build Images
  - name: 'gcr.io/cloud-builders/docker'
    id: 'Build Training Image'
    args: ['build', '-t', '$_REGION-docker.pkg.dev/$PROJECT_ID/dtu-mlops-images/train-image:latest', '--cache-from', '$_REGION-docker.pkg.dev/$PROJECT_ID/dtu-mlops-images/train-image:latest', '-f', 'dockerfiles/train.dockerfile', '.']

  - name: 'gcr.io/cloud-builders/docker'
    id: 'Build API Image'
    args: ['build', '-t', '$_REGION-docker.pkg.dev/$PROJECT_ID/dtu-mlops-images/api-image:latest', '--cache-from', '$_REGION-docker.pkg.dev/$PROJECT_ID/dtu-mlops-images/api-image:latest', '-f', 'dockerfiles/api.dockerfile', '.']

  - name: 'gcr.io/cloud-builders/docker'
    id: 'Build Frontend Image'
    args: ['build', '-t', '$_REGION-docker.pkg.dev/$PROJECT_ID/dtu-mlops-images/frontend-image:latest', '--cache-from', '$_REGION-docker.pkg.dev/$PROJECT_ID/dtu-mlops-images/frontend-image:latest', '-f', 'dockerfiles/frontend.dockerfile', '.']

  # STEP 4-6: Push to Artifact Registry
  - name: 'gcr.io/cloud-builders/docker'
    id: 'Push Training'
    args: ['push', '$_REGION-docker.pkg.dev/$PROJECT_ID/dtu-mlops-images/train-image:latest']

  - name: 'gcr.io/cloud-builders/docker'
    id: 'Push API'
    args: ['push', '$_REGION-docker.pkg.dev/$PROJECT_ID/dtu-mlops-images/api-image:latest']

  - name: 'gcr.io/cloud-builders/docker'
    id: 'Push Frontend'
    args: ['push', '$_REGION-docker.pkg.dev/$PROJECT_ID/dtu-mlops-images/frontend-image:latest']

  # STEP 7: Vertex AI Training Job (Wait for success)
  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    id: 'Trigger Training'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        set -e
        JOB_NAME=$$(gcloud ai custom-jobs create \
          --region=$_REGION \
          --display-name=train-$BUILD_ID \
          --worker-pool-spec=machine-type=n1-standard-4,container-image-uri=$_REGION-docker.pkg.dev/$PROJECT_ID/dtu-mlops-images/train-image:latest \
          --format="value(name)")
        
        while true; do
          STATE=$$(gcloud ai custom-jobs describe $$JOB_NAME --region=$_REGION --format="value(state)")
          echo "Current Job State: $$STATE"
          if [ "$$STATE" = "JOB_STATE_SUCCEEDED" ]; then
            break
          elif [ "$$STATE" = "JOB_STATE_FAILED" ] || [ "$$STATE" = "JOB_STATE_CANCELLED" ]; then
            echo "Training failed!" && exit 1
          fi
          sleep 30
        done

  # STEP 8: Deploy FastAPI (Backend)
  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    id: 'Deploy API'
    entrypoint: 'gcloud'
    args:
      - 'run'
      - 'deploy'
      - 'dtu-mlops-api'
      - '--image=$_REGION-docker.pkg.dev/$PROJECT_ID/dtu-mlops-images/api-image:latest'
      - '--region=$_REGION'
      - '--platform=managed'
      - '--allow-unauthenticated'
      - '--port=8000'
      - '--memory=2Gi'
      - '--cpu=2'
      - '--timeout=300'
      - '--cpu-boost'

  # STEP 9: Deploy Streamlit (Frontend)
  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    id: 'Deploy Frontend'
    entrypoint: 'gcloud'
    args:
      - 'run'
      - 'deploy'
      - 'dtu-mlops-frontend'
      - '--image=$_REGION-docker.pkg.dev/$PROJECT_ID/dtu-mlops-images/frontend-image:latest'
      - '--region=$_REGION'
      - '--platform=managed'
      - '--allow-unauthenticated'
      - '--port=8501'
      - '--cpu-boost'
      - '--timeout=300'

options:
  logging: CLOUD_LOGGING_ONLY
  machineType: 'E2_HIGHCPU_8'

timeout: 7200s

substitutions:
  _REGION: europe-west1